{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_FROM_APACHE-SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName('DataAnalysis').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+------+----------+---------+-----------+------------+----------+--------------+--------+--------+----------+---+---+---+-----+-----------+-------+------+-----------+---+\n",
      "|student|gender|age|region|traveltime|studytime|performance|parentincome|activities|educatedparent|internet|freetime|attendance| G1| G2| G3|total| percentage|percent|result|shortlisted|inc|\n",
      "+-------+------+---+------+----------+---------+-----------+------------+----------+--------------+--------+--------+----------+---+---+---+-----+-----------+-------+------+-----------+---+\n",
      "|      1|female| 18| urban|         2|        2|         60|         450|         0|             1|       1|       3|        63|  5|  6|  6|   18|       30.0|   30.0|     1|          0|100|\n",
      "|      2|female| 17| urban|         1|        1|         40|         100|         0|             0|       0|       3|        83|  5|  5|  6|   16|26.66666667|   26.0|     0|          1| 65|\n",
      "|      3|female| 15| urban|         1|        2|         60|          65|         0|             0|       0|       3|        51|  7|  8| 10|   26|43.33333333|   43.0|     1|          1|300|\n",
      "|      4|female| 15| urban|         1|        3|         80|         300|         1|             1|       1|       2|        94| 15| 14| 15|   45|       75.0|   75.0|     1|          0|400|\n",
      "|      5|female| 16| urban|         1|        4|         90|         500|         0|             1|       1|       3|        76|  6| 10| 10|   27|       45.0|   45.0|     1|          0| 80|\n",
      "|      6|  male| 16| urban|         1|        2|         60|         400|         1|             1|       1|       4|        94| 15| 15| 15|   46|76.66666667|   76.0|     1|          0|200|\n",
      "|      7|  male| 16| urban|         1|        2|         60|         500|         0|             1|       1|       4|        93| 12| 12| 11|   36|       60.0|   60.0|     1|          0|450|\n",
      "|      8|female| 17| urban|         2|        4|         90|          70|         0|             0|       0|       1|        86|  6|  5|  6|   17|28.33333333|   28.0|     0|          1|450|\n",
      "|      9|  male| 15| urban|         1|        2|         60|         400|         0|             1|       1|       2|        57| 16| 18| 19|   54|       90.0|   90.0|     1|          0|500|\n",
      "|     10|  male| 15| urban|         1|        3|         80|         300|         1|             1|       1|       5|        64| 14| 15| 15|   45|       75.0|   75.0|     1|          0|450|\n",
      "+-------+------+---+------+----------+---------+-----------+------------+----------+--------------+--------+--------+----------+---+---+---+-----+-----------+-------+------+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV IN SPARK-DATAFRAME\n",
    "\n",
    "df = spark.read.csv(\"C:/GITHUB_PROJECTS/StudDA_ML/student.csv\", header=True, inferSchema=True)\n",
    "df = df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Fill missing values with a specific value\n",
    "df = df.fillna({'age': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregating: 'groupBy()' & 'avg(), count(), sum()'\n",
    "# avg\n",
    "df.groupBy('gender').agg({'age': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "df.groupBy('gender').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLORATORU_DATA_ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import plotly.express as px\n",
    "\n",
    "# Group by age and count the number of students\n",
    "grouped_df = df.groupBy(\"age\").count()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df = grouped_df.toPandas()\n",
    "\n",
    "# Create bar plot with Plotly\n",
    "fig = px.bar(pandas_df, x='age', y='count', title='Student Count by Age')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MACHINE LEARNING\n",
    "LOGISTIC_CLASSIFICATION_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant columns\n",
    "data = data.select(\"G1\", \"G2\", \"G3\", \"pass\")\n",
    "\n",
    "# create feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"G1\", \"G2\", \"G3\"], outputCol=\"features\")\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# train logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"pass\", maxIter=10, regParam=0.01)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# make predictions on test data\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"pass\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "\n",
    "# show confusion matrix\n",
    "predictions.groupBy(\"pass\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on new data\n",
    "new_data = spark.read.csv(\"C:/nasoindiadev/interns-20222023/dataanalytics/code/data/new_stud.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# select relevant columns\n",
    "new_data = new_data.select(\"G1\", \"G2\", \"G3\")\n",
    "\n",
    "# create feature vector\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "# make predictions\n",
    "new_predictions = lr_model.transform(new_data)\n",
    "\n",
    "# show predictions with all columns\n",
    "new_predictions.select(\"*\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
